{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a3d4a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 01:46:15.052943: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 01:46:15.052967: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 01:46:19.551355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-09 01:46:19.551389: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-09 01:46:19.551414: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-09 01:46:19.551676: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 01:46:19.567165: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.9.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.4.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# GPTv2 모델 파인튜닝 & 저장 : 약 3시간 걸림 (8 CPU & 0 GPU)\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('한글성경들(마침표제거)_정제후말뭉치_약백만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "# Define the training parameters\n",
    "model_path = \"./output/gpt2-finetuned-epoch-66\"\n",
    "learning_rate = 3e-5\n",
    "batch_size = 16\n",
    "epochs = 66\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # \"1024\" for GPT-2\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03b1dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 04:47:02.218296: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 04:47:02.218325: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 04:47:04.467570: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-09 04:47:04.467593: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-09 04:47:04.467610: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-09 04:47:04.467818: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 04:47:04.480476: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835  7177]], shape=(1, 2), dtype=int32)\n",
      "[17835, 7177, 23916, 24117, 8702, 7816, 16913, 7182, 9051, 6947, 7399, 7220, 9022, 20085, 9782, 9582, 28608, 9080, 16082, 9320, 13801, 11920, 9402, 10433, 31825, 23678, 9414, 8006, 25856, 28732, 8146, 11404, 24487, 20776, 18406, 35420, 36861, 11792, 7372, 16256, 19300, 9157, 9612, 16157, 6889, 14618, 9960, 50339, 10645, 9926, 9651, 10010, 39514, 8137, 32389, 17339, 9018, 20767, 14684, 9673, 9661, 14782, 11698, 19495, 44111, 9169, 13721, 10536, 9466, 9142, 7810, 7788, 11947, 28797, 10781, 21511, 17969, 6903, 34935, 28056, 13183, 8143, 18636, 10106, 8159, 11403, 11091, 24752, 16107, 9036, 14247, 9079, 10280, 739, 7567, 8135, 10551, 9375, 9078, 35180, 10554, 9481, 18961, 22163, 23567, 9710, 387, 12082, 9585, 7807, 8286, 41878, 9108, 9306, 33136, 15312, 16691, 23354, 9034, 7489, 13927, 37202, 9176, 10070, 14143, 19749, 9564, 11936]\n",
      "예수님께서 말씀하셨습니다 \n",
      "그런데 그 빛이 지금처럼 짜임새 있는 모습이 아니었고 생물 하나 없이 텅 비어 있었어요. 어둠이 깊은 바다를 덮고 있었고 빛과 그림자가 생겨라 그러자 빛의 형성이 생겼고 빛을 낮 이라 부르기로 했습니다.\n",
      "저녁을 지나고 아침 이 날이 첫째 날이었어요.\n",
      "하나님의 영은 물 위에서 움직이고 계셔서 물을 둘로 나누어라, 저녁과 밤을 나누고 계절의 흐름을 나누자 그래서 하늘 아래의 물은 한 곳으로 모이고, 뭍은 드러나라 하시니 그대로 되었지요.\n",
      "하나, 하나, 둘, 셋.\n",
      "셋째 날은 또 다른 날에 일어났습니다. 며칠 전만 해도 그랬지만 오늘만은 상황이 많이 달라\n",
      "Time duration(in seconds): 31.048101376974955\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 생성\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-66\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "start = default_timer()\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "generated_ids = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # 1.0 indicates no penalty for repeating tokens, up to the 2.0\n",
    "                        num_return_sequences=1, # the number of independent sequences to generate for each prompt\n",
    "                        early_stopping=True, # stops generating a sentence before max_length working with eos_token_id\n",
    "                        use_cache=True, # enables or disables the use of the model's internal cache (repetitive output)\n",
    "                        eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d03396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835 20108 14938  9329  9658 19202   406]], shape=(1, 7), dtype=int32)\n",
      "[17835, 20108, 14938, 9329, 9658, 19202, 406, 16563, 6947, 7399, 7220, 9022, 11146, 9782, 9582, 28608, 9080, 16082, 9320, 13801, 11920, 9402, 10433, 31825, 23678, 9414, 16913, 7182, 28732, 8146, 11404, 24487, 20776, 18406, 9169, 13721, 16235, 41701, 9564, 17828, 10010, 47637, 9835, 9177, 9036, 14247, 9079, 9466, 739, 7567, 8135, 10551, 9375, 9078, 35180, 10554, 9481, 16691, 23354, 9133, 9018, 20767, 14684, 9673, 9661, 18961, 11698, 7177, 23916, 24117, 8702, 7816, 6889, 9108, 9135, 8718, 8017, 8006, 25856, 9051, 7261, 8286, 41878, 40340, 8149, 24917, 17339, 6903, 17969, 8137, 10106, 7530, 12859, 8263, 10171, 14782, 39514, 9052, 12503, 34677, 9142, 7810, 7788, 10578, 9049, 7888, 7251, 13675, 7807, 43143, 11091, 9793, 12611, 24186, 11792, 11947, 28797, 10781, 7372, 16256, 14927, 39417, 34693, 24389, 23295, 10027, 11608, 24860]\n",
      "예수님을 누구라고 생각하니?\"\n",
      "그런데 그 사람은 지금처럼 짜임새 있는 모습이 아니었고 생물 하나 없이 텅 비어 있었습니다 어둠이 깊은 바다를 덮고 있었고 물 위에서 움직이는 생물을 많이 보았습니다.\n",
      "그러나 그것들은 한 곳으로 모이고 뭍은 드러나라 하시니 그대로 되었습니다. 며칠 후 이 날이 첫째 날이었지요.\n",
      "하나님께서 말씀하셨고 또 말했었어요. \n",
      "둘째 날은 안식일이었고 아침과 저녁을 나누며 지냈지 않았어요.\n",
      "저녁에는 쉬시고 계셔서 일을 보시되요?\n",
      "셋째로 하늘 아래 둥근 공간이 생겨 물을 둘로 나누어라 그러자 그렇게 됐죠.\n",
      "우주 공간에 떠있는 것들은\n",
      "Time duration(in seconds): 34.9103637279477\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 생성\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-66\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님을 누구라고 생각하니?'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "start = default_timer()\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "generated_ids = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # 1.0 indicates no penalty for repeating tokens, up to the 2.0\n",
    "                        num_return_sequences=1, # the number of independent sequences to generate for each prompt\n",
    "                        early_stopping=True, # stops generating a sentence before max_length working with eos_token_id\n",
    "                        use_cache=True, # enables or disables the use of the model's internal cache (repetitive output)\n",
    "                        eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588a5035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current date and time:  2023-03-09 04:48:39.625730\n"
     ]
    }
   ],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "print(\"Current date and time: \", datetime_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
