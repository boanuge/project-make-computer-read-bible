{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)\n",
    "\n",
    "# How many epochs needs to be interated to get the best result on DialoGPT-large model\n",
    "# when finetuning with one million words of new dataset?\n",
    "'''\n",
    "[ChatGPT & BingChat] Based on the experiences shared in the GitHub repository of the Hugging-Face team,\n",
    "fine-tuning the Microsoft DialoGPT-large model with one million words requires at least several hundred epochs,\n",
    "typically around 2000 epochs using a batch size of 2, because the model has a large number of parameters (774 million),\n",
    "or until the validation loss plateaus. For example:\n",
    "\n",
    "train_dataset = ... # create or load your training dataset\n",
    "batch_size = 2\n",
    "model.compile(loss=model.compute_loss, optimizer=\"adam\")\n",
    "model.fit(train_dataset.batch(batch_size), \n",
    "          validation_data=train_dataset.batch(batch_size),\n",
    "          batch_size=batch_size, \n",
    "          epochs=2000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-26T18:14:25.248695)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8581266 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-26T18:14:49.050494)\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 61, in <cell line: 57>\n      loss_value = train_step(batch)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 40, in train_step\n      outputs = model(input_ids, training=True)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 727, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 373, in call\n      for i, (block, layer_past) in enumerate(zip(self.h, inputs[\"past\"])):\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 377, in call\n      outputs = block(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 199, in call\n      output_attn = self.attn(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 160, in call\n      attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 120, in _attn\n      w = self.attn_dropout(w, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[1,20,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_202259]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mg:\\AI\\NLP DialoGPT (TF) via Bible (한글성경) in Korean.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     loss_value \u001b[39m=\u001b[39m train_step(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{\u001b[39;00mloss_value\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 61, in <cell line: 57>\n      loss_value = train_step(batch)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 40, in train_step\n      outputs = model(input_ids, training=True)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 727, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 373, in call\n      for i, (block, layer_past) in enumerate(zip(self.h, inputs[\"past\"])):\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 377, in call\n      outputs = block(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 199, in call\n      output_attn = self.attn(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 160, in call\n      attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 120, in _attn\n      w = self.attn_dropout(w, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[1,20,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_202259]"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer\n",
    "\n",
    "model_path = \"DialoGPT-large-finetuned-by-Microsoft-Keras\"\n",
    "learning_rate = 1e-5\n",
    "batch_size = 2\n",
    "epochs = 2000\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "# Load the text data\n",
    "with open('영어성경(NKJV+NIV+NLT)_이백만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "\n",
    "print_current_datetime(\"Finetune the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # \"1024\" for DialoGPT-large\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print_current_datetime(\"Saving Fine-tuned Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from timeit import default_timer\n",
    "\n",
    "model_path = \"DialoGPT-large-finetuned-by-Microsoft-Keras\"\n",
    "\n",
    "print_current_datetime(\"Loading Fine-tuned Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(model_path)\n",
    "model_loaded = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def generate_ouput(prompt=\"\"):\n",
    "\n",
    "    start = default_timer()\n",
    "\n",
    "    input_ids = tokenizer_loaded.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = model_loaded.generate(input_ids=input_ids,\n",
    "                                       max_length=1024+input_ids.shape[1],\n",
    "                                       temperature=0.7,\n",
    "                                       top_p=0.9,\n",
    "                                       do_sample=True,\n",
    "                                       num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "                                       pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "    generated_text = tokenizer_loaded.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    end = default_timer()\n",
    "\n",
    "    # num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "    # The below code loops through the generated responses and print them out with a response number.\n",
    "    for i, return_sequence in enumerate(output_ids):\n",
    "        print(f'Response {i+1}: {tokenizer_loaded.decode(return_sequence, skip_special_tokens=True)}')\n",
    "\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "\n",
    "    generated_text = generate_ouput(prompt)\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "\n",
    "    print(\">> GPT: {}\".format( generated_text ))\n",
    "    print_current_datetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained microsoft/DialoGPT-large @ CDT(2023-04-03T20:46:16.762737)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0d681fbf81465bba5f79b86c9096e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\A\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa4ed15fcb84668a3488a7fce298077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de6d6592604903ab378862eb34f6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35538ba5352466f947f17934f437db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c26104d454a4549b22af7f1271a37fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/3.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8222207747e4924bf36ac99b02bc66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-03T20:51:42.969294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi and I'm a big fan of your videos\n",
      "Response 1: Hi and I'm a big fan of your videos\n",
      "Response 2: Hi\n",
      "Response 3: Hi\n",
      "Response 4: Hi\n",
      "Response 5: Hi\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Loading pre-trained microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids,\n",
    "    max_length=1024,\n",
    "    temperature=0.7, # This parameter controls the \"creativity\" of the generated text, \"1\" results unpredictable text\n",
    "    top_p=0.7, # Cumulative probability distribution of the next word, \"top_k\": limits the number of candidate words \n",
    "    do_sample=True, # Random sampling of the next token instead of selecting the token with the highest probability \n",
    "    num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "print(loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "# The below code loops through the generated responses and print them out with a response number.\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:30:15.742516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-04 07:30:15.742551: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T07:30:17.126496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:30:17.926118: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-04 07:30:17.926146: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-04 07:30:17.926165: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-04-04 07:30:17.926362: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 07:30:17.938886: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated-text:  you, welcome to the club!\n",
      "Response 1: Hi you, welcome to the club!\n",
      "Response 2: Hi\n",
      "Response 3: Hi\n",
      "Response 4: Hi.\n",
      "Response 5: Hi :D\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "split_generated_text = generated_text.split(prompt)\n",
    "if len(split_generated_text) > 1:\n",
    "    generated_text = split_generated_text[1]\n",
    "\n",
    "print(\"Generated-text:\",generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune(Before) the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T07:51:44.185421)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus @ CDT(2023-04-04T07:52:15.653434)\n",
      "Response 1: Jesus\n",
      "Response 2: Jesus... the title was a little unclear.\n",
      "Response 3: Jesus\n",
      "Response 4: Jesus a... it's a trap.\n",
      "Response 5: Jesus, is there any chance you could turn off the game from your home computer. It is your PC's fault, you can't make a computer turn itself on.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Finetune(Before) the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Jesus\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print_current_datetime(generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3054759 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 3054759), dtype=int32, numpy=array([[ 818,  262, 3726, ...,  345,  477,   13]])>, 'attention_mask': <tf.Tensor: shape=(1, 3054759), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]])>}\n",
      "Finetune the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T17:47:04.236880)\n",
      "Epoch 1/6\n",
      "Step 0 Loss 12.522553443908691\n",
      "Time duration(in seconds): 74.71662520000001\n",
      "Epoch 2/6\n",
      "Step 0 Loss 10.721572875976562\n",
      "Time duration(in seconds): 30.79476849999999\n",
      "Epoch 3/6\n",
      "Step 0 Loss 10.05924129486084\n",
      "Time duration(in seconds): 31.29861169999998\n",
      "Epoch 4/6\n",
      "Step 0 Loss 9.5293607711792\n",
      "Time duration(in seconds): 31.979575799999992\n",
      "Epoch 5/6\n",
      "Step 0 Loss 9.15305233001709\n",
      "Time duration(in seconds): 31.681460799999996\n",
      "Epoch 6/6\n",
      "Step 0 Loss 8.982477188110352\n",
      "Time duration(in seconds): 31.39858380000001\n",
      " @ CDT(2023-04-04T17:52:13.947517)\n",
      "Finetune(After) the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T17:52:13.948516)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus @ CDT(2023-04-04T17:52:32.831162)\n",
      "Response 1: Jesus\n",
      "Response 2: Jesus the that's not how it works.\n",
      "Response 3: Jesus on dude\n",
      "Response 4: Jesus!, i want to learn Spanish\n",
      "Response 5: Jesus\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('영어성경(NKJV+NIV+NLT)_이백만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the model\n",
    "model = TFAutoModelForCausalLM.from_pretrained('DialoGPT-large-by-Microsoft-Keras')\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('DialoGPT-large-by-Microsoft-Keras')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "print_current_datetime(\"Finetune the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "# Define the training parameters\n",
    "model_path = \"DialoGPT-large-by-Microsoft-Keras-finetuned\"\n",
    "learning_rate = 1e-5\n",
    "batch_size = 6\n",
    "epochs = 6\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # \"1024\" for DialoGPT-large\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "print_current_datetime(\"Finetune(After) the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras-finetuned\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras-finetuned\")\n",
    "\n",
    "prompt = \"Jesus\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print_current_datetime(generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
