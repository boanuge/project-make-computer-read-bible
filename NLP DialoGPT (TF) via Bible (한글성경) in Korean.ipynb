{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fd5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)\n",
    "\n",
    "# How many epochs needs to be interated to get the best result on DialoGPT-large model\n",
    "# when finetuning with one million words of new dataset?\n",
    "'''\n",
    "[ChatGPT & BingChat] Based on the experiences shared in the GitHub repository of the Hugging-Face team,\n",
    "fine-tuning the Microsoft DialoGPT-large model with one million words requires at least several hundred epochs,\n",
    "typically around 2000 epochs using a batch size of 2, because the model has a large number of parameters (774 million),\n",
    "or until the validation loss plateaus. For example:\n",
    "\n",
    "train_dataset = ... # create or load your training dataset\n",
    "batch_size = 2\n",
    "model.compile(loss=model.compute_loss, optimizer=\"adam\")\n",
    "model.fit(train_dataset.batch(batch_size), \n",
    "          validation_data=train_dataset.batch(batch_size),\n",
    "          batch_size=batch_size, \n",
    "          epochs=2000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c02a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer\n",
    "\n",
    "model_path = \"DialoGPT-large-finetuned-by-Microsoft-Keras\"\n",
    "batch_size = 2\n",
    "epochs = 2000\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "# Load the text data\n",
    "with open('한글성경(마침표제거)_정제후말뭉치_백일만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print_current_datetime(\"Tokenizing the text\")\n",
    "\n",
    "text_tokenized = tokenizer.encode(text, return_tensors='tf')\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(text_tokenized['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "print_current_datetime(\"Preparing the dataset\")\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss=[loss, loss])\n",
    "\n",
    "model.fit(dataset,\n",
    "          validation_data=dataset,\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs)\n",
    "\n",
    "print_current_datetime(\"Saving Fine-tuned Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf1c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from timeit import default_timer\n",
    "\n",
    "model_path = \"DialoGPT-large-finetuned-by-Microsoft-Keras\"\n",
    "\n",
    "print_current_datetime(\"Loading Fine-tuned Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(model_path)\n",
    "model_loaded = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def generate_ouput(prompt=\"\"):\n",
    "\n",
    "    start = default_timer()\n",
    "\n",
    "    input_ids = tokenizer_loaded.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = model_loaded.generate(input_ids=input_ids,\n",
    "                                       max_length=1024+input_ids.shape[1],\n",
    "                                       temperature=0.7,\n",
    "                                       top_p=0.9,\n",
    "                                       do_sample=True,\n",
    "                                       num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "                                       pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "    generated_text = tokenizer_loaded.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    end = default_timer()\n",
    "\n",
    "    # num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "    # The below code loops through the generated responses and print them out with a response number.\n",
    "    for i, return_sequence in enumerate(output_ids):\n",
    "        print(f'Response {i+1}: {tokenizer_loaded.decode(return_sequence, skip_special_tokens=True)}')\n",
    "\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "\n",
    "    generated_text = generate_ouput(prompt)\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "\n",
    "    print(\">> GPT: {}\".format( generated_text ))\n",
    "    print_current_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
