{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.10.0 #tensorflow 2.10.0 <-- 2.12.0 (버전 다운그레이드 필요)\n",
    "\n",
    "# How many epochs needs to be interated to get the best result on DialoGPT-large model\n",
    "# when finetuning with one million words of new dataset?\n",
    "'''\n",
    "[ChatGPT & BingChat] Based on the experiences shared in the GitHub repository of the Hugging-Face team,\n",
    "fine-tuning the Microsoft DialoGPT-large model with one million words requires at least several hundred epochs,\n",
    "typically around 2000 epochs using a batch size of 2, because the model has a large number of parameters (774 million),\n",
    "or until the validation loss plateaus. For example:\n",
    "\n",
    "train_dataset = ... # create or load your training dataset\n",
    "batch_size = 2\n",
    "model.compile(loss=model.compute_loss, optimizer=\"adam\")\n",
    "model.fit(train_dataset.batch(batch_size), \n",
    "          validation_data=train_dataset.batch(batch_size),\n",
    "          batch_size=batch_size, \n",
    "          epochs=2000)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @ CDT(2023-04-26T18:14:25.248695)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8581266 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-26T18:14:49.050494)\n",
      "Epoch 1/2000\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 61, in <cell line: 57>\n      loss_value = train_step(batch)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 40, in train_step\n      outputs = model(input_ids, training=True)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 727, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 373, in call\n      for i, (block, layer_past) in enumerate(zip(self.h, inputs[\"past\"])):\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 377, in call\n      outputs = block(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 199, in call\n      output_attn = self.attn(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 160, in call\n      attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 120, in _attn\n      w = self.attn_dropout(w, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[1,20,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_202259]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mg:\\AI\\NLP DialoGPT (TF) via Bible (한글성경) in Korean.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     loss_value \u001b[39m=\u001b[39m train_step(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/AI/NLP%20DialoGPT%20%28TF%29%20via%20Bible%20%28%ED%95%9C%EA%B8%80%EC%84%B1%EA%B2%BD%29%20in%20Korean.ipynb#W0sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mStep \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{\u001b[39;00mloss_value\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform' defined at (most recent call last):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n      self.io_loop.start()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n      await self.process_one()\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n      await dispatch(*args)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n      await result\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n      reply_content = await reply_content\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n      result = self._run_cell(\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n      return runner(coro)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 61, in <cell line: 57>\n      loss_value = train_step(batch)\n    File \"C:\\Windows\\Temp\\ipykernel_11040\\3663138582.py\", line 40, in train_step\n      outputs = model(input_ids, training=True)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 727, in call\n      transformer_outputs = self.transformer(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 373, in call\n      for i, (block, layer_past) in enumerate(zip(self.h, inputs[\"past\"])):\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 377, in call\n      outputs = block(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 199, in call\n      output_attn = self.attn(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 160, in call\n      attn_outputs = self._attn(query, key, value, attention_mask, head_mask, output_attentions, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py\", line 120, in _attn\n      w = self.attn_dropout(w, training=training)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 116, in call\n      output = control_flow_util.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\control_flow_util.py\", line 108, in smart_cond\n      return tf.__internal__.smart_cond.smart_cond(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\regularization\\dropout.py\", line 112, in dropped_inputs\n      return self._random_generator.dropout(\n    File \"C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\backend.py\", line 2162, in dropout\n      return tf.nn.dropout(\nNode: 'tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform'\nOOM when allocating tensor with shape[1,20,1024,1024] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node tfgpt2lm_head_model_8/transformer/h_._33/attn/dropout_972/dropout/random_uniform/RandomUniform}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_step_202259]"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer\n",
    "\n",
    "model_path = \"DialoGPT-large-finetuned-by-Microsoft-Keras\"\n",
    "learning_rate = 1e-5\n",
    "batch_size = 2\n",
    "epochs = 2000\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "# Load the text data\n",
    "with open('한글성경(마침표제거)_정제후말뭉치_백일만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "\n",
    "print_current_datetime(\"Finetune the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # \"1024\" for DialoGPT-large\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print_current_datetime(\"Saving Fine-tuned Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer.save_pretrained(model_path)\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from timeit import default_timer\n",
    "\n",
    "model_path = \"DialoGPT-large-finetuned-by-Microsoft-Keras\"\n",
    "\n",
    "print_current_datetime(\"Loading Fine-tuned Microsoft DialoGPT Model\")\n",
    "\n",
    "tokenizer_loaded = AutoTokenizer.from_pretrained(model_path)\n",
    "model_loaded = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "def generate_ouput(prompt=\"\"):\n",
    "\n",
    "    start = default_timer()\n",
    "\n",
    "    input_ids = tokenizer_loaded.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = model_loaded.generate(input_ids=input_ids,\n",
    "                                       max_length=1024+input_ids.shape[1],\n",
    "                                       temperature=0.7,\n",
    "                                       top_p=0.9,\n",
    "                                       do_sample=True,\n",
    "                                       num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "                                       pad_token_id=tokenizer_loaded.eos_token_id)\n",
    "    generated_text = tokenizer_loaded.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    end = default_timer()\n",
    "\n",
    "    # num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "    # The below code loops through the generated responses and print them out with a response number.\n",
    "    for i, return_sequence in enumerate(output_ids):\n",
    "        print(f'Response {i+1}: {tokenizer_loaded.decode(return_sequence, skip_special_tokens=True)}')\n",
    "\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    return generated_text\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "\n",
    "    generated_text = generate_ouput(prompt)\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "\n",
    "    print(\">> GPT: {}\".format( generated_text ))\n",
    "    print_current_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
