{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19a3d4a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 22:47:07.988091: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-07 22:47:07.988115: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-07 22:47:07.988132: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-07 22:47:07.988364: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-07 22:47:08.001102: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.6.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'lm_head.weight', 'transformer.h.9.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 1042061), dtype=int32, numpy=array([[ 9342,   392, 20252, ...,  9050,  7523,   389]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 1042061), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n",
      "Epoch 1/66\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5bc777ba00>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f5bc777ba00>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Step 0 Loss 2.855166435241699\n",
      "Time duration(in seconds): 15.320759003981948\n",
      "Epoch 2/66\n",
      "Step 0 Loss 2.426194906234741\n",
      "Time duration(in seconds): 4.289433935016859\n",
      "Epoch 3/66\n",
      "Step 0 Loss 2.0378119945526123\n",
      "Time duration(in seconds): 4.177872560976539\n",
      "Epoch 4/66\n",
      "Step 0 Loss 1.7339463233947754\n",
      "Time duration(in seconds): 4.148639220977202\n",
      "Epoch 5/66\n",
      "Step 0 Loss 1.517874836921692\n",
      "Time duration(in seconds): 4.04906921897782\n",
      "Epoch 6/66\n",
      "Step 0 Loss 1.346970796585083\n",
      "Time duration(in seconds): 4.039101052971091\n",
      "Epoch 7/66\n",
      "Step 0 Loss 1.1385786533355713\n",
      "Time duration(in seconds): 3.937368548999075\n",
      "Epoch 8/66\n",
      "Step 0 Loss 1.016096591949463\n",
      "Time duration(in seconds): 4.021152678993531\n",
      "Epoch 9/66\n",
      "Step 0 Loss 0.8840915560722351\n",
      "Time duration(in seconds): 3.9739622729830444\n",
      "Epoch 10/66\n",
      "Step 0 Loss 0.7722957730293274\n",
      "Time duration(in seconds): 3.9800529219792224\n",
      "Epoch 11/66\n",
      "Step 0 Loss 0.6611809730529785\n",
      "Time duration(in seconds): 4.020497509976849\n",
      "Epoch 12/66\n",
      "Step 0 Loss 0.5778843760490417\n",
      "Time duration(in seconds): 3.911775814020075\n",
      "Epoch 13/66\n",
      "Step 0 Loss 0.49335020780563354\n",
      "Time duration(in seconds): 4.084396030986682\n",
      "Epoch 14/66\n",
      "Step 0 Loss 0.4187007546424866\n",
      "Time duration(in seconds): 3.910902123025153\n",
      "Epoch 15/66\n",
      "Step 0 Loss 0.3653377592563629\n",
      "Time duration(in seconds): 4.0097367789712735\n",
      "Epoch 16/66\n",
      "Step 0 Loss 0.3120872974395752\n",
      "Time duration(in seconds): 3.9776662919903174\n",
      "Epoch 17/66\n",
      "Step 0 Loss 0.24861657619476318\n",
      "Time duration(in seconds): 3.9757573429960757\n",
      "Epoch 18/66\n",
      "Step 0 Loss 0.23058363795280457\n",
      "Time duration(in seconds): 3.990094640990719\n",
      "Epoch 19/66\n",
      "Step 0 Loss 0.20871485769748688\n",
      "Time duration(in seconds): 3.9466481509734876\n",
      "Epoch 20/66\n",
      "Step 0 Loss 0.16927357017993927\n",
      "Time duration(in seconds): 3.8757807349902578\n",
      "Epoch 21/66\n",
      "Step 0 Loss 0.15938644111156464\n",
      "Time duration(in seconds): 4.049907913955394\n",
      "Epoch 22/66\n",
      "Step 0 Loss 0.13443522155284882\n",
      "Time duration(in seconds): 3.975218589999713\n",
      "Epoch 23/66\n",
      "Step 0 Loss 0.1323276162147522\n",
      "Time duration(in seconds): 3.904004586976953\n",
      "Epoch 24/66\n",
      "Step 0 Loss 0.11655738949775696\n",
      "Time duration(in seconds): 3.9815471619949676\n",
      "Epoch 25/66\n",
      "Step 0 Loss 0.09991341829299927\n",
      "Time duration(in seconds): 3.9646952530019917\n",
      "Epoch 26/66\n",
      "Step 0 Loss 0.08847595006227493\n",
      "Time duration(in seconds): 4.020201392995659\n",
      "Epoch 27/66\n",
      "Step 0 Loss 0.08157560974359512\n",
      "Time duration(in seconds): 3.953375835029874\n",
      "Epoch 28/66\n",
      "Step 0 Loss 0.07729709148406982\n",
      "Time duration(in seconds): 3.980336904001888\n",
      "Epoch 29/66\n",
      "Step 0 Loss 0.06872625648975372\n",
      "Time duration(in seconds): 3.9334895299980417\n",
      "Epoch 30/66\n",
      "Step 0 Loss 0.06733175367116928\n",
      "Time duration(in seconds): 3.922408076992724\n",
      "Epoch 31/66\n",
      "Step 0 Loss 0.0560123585164547\n",
      "Time duration(in seconds): 3.9730861069983803\n",
      "Epoch 32/66\n",
      "Step 0 Loss 0.05167210102081299\n",
      "Time duration(in seconds): 4.012560573988594\n",
      "Epoch 33/66\n",
      "Step 0 Loss 0.04852038621902466\n",
      "Time duration(in seconds): 4.077504623041023\n",
      "Epoch 34/66\n",
      "Step 0 Loss 0.04438000172376633\n",
      "Time duration(in seconds): 3.95260766800493\n",
      "Epoch 35/66\n",
      "Step 0 Loss 0.03886004909873009\n",
      "Time duration(in seconds): 3.959601244016085\n",
      "Epoch 36/66\n",
      "Step 0 Loss 0.03751752898097038\n",
      "Time duration(in seconds): 4.067543244978879\n",
      "Epoch 37/66\n",
      "Step 0 Loss 0.035700563341379166\n",
      "Time duration(in seconds): 3.9784963940037414\n",
      "Epoch 38/66\n",
      "Step 0 Loss 0.03561711311340332\n",
      "Time duration(in seconds): 3.985701166035142\n",
      "Epoch 39/66\n",
      "Step 0 Loss 0.032869238406419754\n",
      "Time duration(in seconds): 4.063265466014855\n",
      "Epoch 40/66\n",
      "Step 0 Loss 0.03025876358151436\n",
      "Time duration(in seconds): 4.046750643989071\n",
      "Epoch 41/66\n",
      "Step 0 Loss 0.025585364550352097\n",
      "Time duration(in seconds): 3.9760092310025357\n",
      "Epoch 42/66\n",
      "Step 0 Loss 0.02543962560594082\n",
      "Time duration(in seconds): 4.021027487993706\n",
      "Epoch 43/66\n",
      "Step 0 Loss 0.0281048770993948\n",
      "Time duration(in seconds): 4.037102174013853\n",
      "Epoch 44/66\n",
      "Step 0 Loss 0.027141574770212173\n",
      "Time duration(in seconds): 4.029146560002118\n",
      "Epoch 45/66\n",
      "Step 0 Loss 0.023396793752908707\n",
      "Time duration(in seconds): 4.1291184569709\n",
      "Epoch 46/66\n",
      "Step 0 Loss 0.02030530758202076\n",
      "Time duration(in seconds): 4.056406332005281\n",
      "Epoch 47/66\n",
      "Step 0 Loss 0.02152748592197895\n",
      "Time duration(in seconds): 3.915683856001124\n",
      "Epoch 48/66\n",
      "Step 0 Loss 0.018862495198845863\n",
      "Time duration(in seconds): 4.065148896013852\n",
      "Epoch 49/66\n",
      "Step 0 Loss 0.019686292856931686\n",
      "Time duration(in seconds): 3.988563252030872\n",
      "Epoch 50/66\n",
      "Step 0 Loss 0.0182065237313509\n",
      "Time duration(in seconds): 4.0431373250321485\n",
      "Epoch 51/66\n",
      "Step 0 Loss 0.01589951477944851\n",
      "Time duration(in seconds): 4.04335059504956\n",
      "Epoch 52/66\n",
      "Step 0 Loss 0.016128381714224815\n",
      "Time duration(in seconds): 4.140648791973945\n",
      "Epoch 53/66\n",
      "Step 0 Loss 0.013257716782391071\n",
      "Time duration(in seconds): 4.02149004797684\n",
      "Epoch 54/66\n",
      "Step 0 Loss 0.01414512563496828\n",
      "Time duration(in seconds): 3.9733502590097487\n",
      "Epoch 55/66\n",
      "Step 0 Loss 0.015973620116710663\n",
      "Time duration(in seconds): 4.095917508006096\n",
      "Epoch 56/66\n",
      "Step 0 Loss 0.01381730381399393\n",
      "Time duration(in seconds): 3.8806458929902874\n",
      "Epoch 57/66\n",
      "Step 0 Loss 0.012122795917093754\n",
      "Time duration(in seconds): 3.9686171189532615\n",
      "Epoch 58/66\n",
      "Step 0 Loss 0.012045085430145264\n",
      "Time duration(in seconds): 4.019861305016093\n",
      "Epoch 59/66\n",
      "Step 0 Loss 0.012122307904064655\n",
      "Time duration(in seconds): 4.092326545971446\n",
      "Epoch 60/66\n",
      "Step 0 Loss 0.012431854382157326\n",
      "Time duration(in seconds): 4.017457577981986\n",
      "Epoch 61/66\n",
      "Step 0 Loss 0.010411227121949196\n",
      "Time duration(in seconds): 3.960171124956105\n",
      "Epoch 62/66\n",
      "Step 0 Loss 0.013260348699986935\n",
      "Time duration(in seconds): 3.9849355340120383\n",
      "Epoch 63/66\n",
      "Step 0 Loss 0.009429523721337318\n",
      "Time duration(in seconds): 4.010711012990214\n",
      "Epoch 64/66\n",
      "Step 0 Loss 0.00885455496609211\n",
      "Time duration(in seconds): 3.9118537990143523\n",
      "Epoch 65/66\n",
      "Step 0 Loss 0.009295995347201824\n",
      "Time duration(in seconds): 4.039380764996167\n",
      "Epoch 66/66\n",
      "Step 0 Loss 0.011178739368915558\n",
      "Time duration(in seconds): 3.9789466690272093\n"
     ]
    }
   ],
   "source": [
    "# GPTv2 모델 파인튜닝 & 저장 : 약 1시간 걸림\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('bible_korean_easy.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "# Define the training parameters\n",
    "model_path = \"./output/gpt2-finetuned-epoch-66\"\n",
    "max_seq_length = 1024 # Usually 1024 for GPT-2\n",
    "learning_rate = 3e-5\n",
    "batch_size = 16\n",
    "epochs = 66\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = max_seq_length\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "    \n",
    "    # Save the fine-tuned model\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03b1dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835  7177]], shape=(1, 2), dtype=int32)\n",
      "[17835, 7177, 23916, 24117, 8702, 7816, 16691, 9640, 16478, 11792, 7372, 50075, 16256, 20085, 16157, 10010, 8344, 392, 47714, 10910, 9022, 10503, 9136, 9402, 19495, 15172, 9506, 12219, 8744, 16913, 7182, 36500, 34265, 9020, 401, 394, 11993, 9049, 7888, 11597, 10554, 9481, 6921, 10078, 26415, 9045, 20252, 9724, 24381, 9347, 405, 9298, 13966, 29335, 22366, 9659, 8210, 8017, 18961, 49084, 9342, 393, 35110, 9108, 9306, 12315, 387, 35420, 28732, 8146, 13926, 6866, 9078, 35180, 6969, 26764, 18911, 9937, 11257, 9085, 40708, 10917, 10106, 7172, 21598, 33835, 9106, 14858, 7532, 9018, 20767, 14684, 9673, 9661, 13083, 9135, 10542, 22294, 12997, 12351, 9130, 9394, 9143, 6824, 9835, 21154, 8, 12199, 8711, 10033, 13805, 21734, 9563, 20056, 35739, 7244, 9933, 31569, 9023, 13363, 9172, 10470, 10802, 9457, 18910, 8658, 7401, 681]\n",
      "예수님께서 말씀하셨습니다. “빛이 생겨라!” 그러자 빛이 생겼습니다.\n",
      "창1:2 그런데 그 빛들이 하나님의 형상대로 뽑혔습니다!\n",
      "창은 1:3 그것을 보시니, 그대로 되었군요.\n",
      "창이 2:1 <세계의 시작> 태초에 하늘과 땅이 만들어졌었지요.\n",
      "창과 창2:18 또 다른 하나는, 빛과 어둠이 나뉘게 하시니까요,\n",
      "창을 1, 2, 3:19 이렇게 나누니까.\n",
      "창으로 따지자면 이 날이 첫째 날이었으니 말입니다.\n",
      "창,2, 3, 4 그리고 5가 그것입니다. 지난해 12월 31일 오후 서울 중구 장충동 신라호텔에서 열린 ‘2015 세계지식포럼’\n",
      "Time duration(in seconds): 30.924905780004337\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 생성\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-66\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "start = default_timer()\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "generated_ids = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # 1.0 indicates no penalty for repeating tokens, up to the 2.0\n",
    "                        num_return_sequences=1, # the number of independent sequences to generate for each prompt\n",
    "                        early_stopping=True, # stops generating a sentence before max_length working with eos_token_id\n",
    "                        use_cache=True, # enables or disables the use of the model's internal cache (repetitive output)\n",
    "                        eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d03396e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/gpt2-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[17835 20108 14938  9329  9658 19202   406]], shape=(1, 7), dtype=int32)\n",
      "[17835, 20108, 14938, 9329, 9658, 19202, 406, 16563, 377, 7561, 7428, 21154, 9402, 7177, 23916, 9049, 35181, 9677, 7978, 16691, 19759, 9022, 14988, 35420, 28732, 8137, 10106, 7816, 6958, 38533, 8344, 392, 47714, 10910, 20085, 11792, 7372, 36500, 34265, 9020, 401, 394, 11993, 11560, 8346, 6889, 14618, 9172, 7067, 18895, 10645, 7888, 9075, 9108, 16878, 739, 6993, 10010, 26415, 14479, 20485, 15584, 15312, 9668, 26764, 18911, 14309, 7283, 9124, 26881, 9701, 17827, 10021, 9018, 9179, 9661, 14782, 33835, 9148, 32810, 9347, 9186, 16913, 9503, 49084, 9342, 393, 20252, 9724, 24381, 47288, 405, 9298, 13966, 29335, 22366, 12541, 7252, 41262, 9782, 9582, 28608, 9080, 16082, 9320, 11409, 11920, 23567, 11153, 7235, 14059, 9583, 23942, 10171, 8017, 18961, 41580, 9045, 40708, 17969, 8146, 32389, 17339, 11352, 20767, 9069, 11597, 10917]\n",
      "예수님을 누구라고 생각하니?\"\n",
      "\"물론입니다. 하나님께서 보시기에 좋았습니다. 왜냐하면 그분은 빛과 어둠을 나누셨기 때문입니다.\n",
      "창1:2 그런데 빛이 생겨라!\n",
      "창은 1:3 그것을 알아채고 빛을 ‘낮’이라 부르시고, 또 불을 껐습니다.\n",
      "창이 일어나자마자 하나가 일어났는데요,\n",
      "창을 든든하게 받쳐 준 이가 바로 이 사람이었어요.\n",
      "창으로부터 빛은 시작되었습니다\n",
      "창과 창2:1 <세계의 첫날> 태초에 하늘과 땅이 창조된 이래, 지금처럼 짜임새 있는 모습이 아니었고, 생물 하나, 공간도 제대로 잡히지 않았었지요.\n",
      "창고 2:19 저녁이 지나고 아침이, 날이 되니, 이렇게\n",
      "Time duration(in seconds): 29.722881061956286\n"
     ]
    }
   ],
   "source": [
    "# GPT 모델 활용 : 문장 생성\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_path = \"./output/gpt2-finetuned-epoch-66\"\n",
    "model = TFGPT2LMHeadModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# GPT가 생성할 문장의 방향성을 알려주기 위한 시작 문자열\n",
    "sent = '예수님을 누구라고 생각하니?'\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(sent)\n",
    "input_ids = tf.convert_to_tensor([input_ids])\n",
    "print(input_ids)\n",
    "\n",
    "start = default_timer()\n",
    "# 정수 시퀀스를 입력받아 GPT가 이어서 문장을 생성 : 약 20초 걸림 (using 1 cpu)\n",
    "generated_ids = model.generate(input_ids, # a tensor containing the input sequence encoded as integer IDs\n",
    "                        max_length=128, # the maximum length of the generated sequence, in terms of tokens\n",
    "                        repetition_penalty=2.0, # 1.0 indicates no penalty for repeating tokens, up to the 2.0\n",
    "                        num_return_sequences=1, # the number of independent sequences to generate for each prompt\n",
    "                        early_stopping=True, # stops generating a sentence before max_length working with eos_token_id\n",
    "                        use_cache=True, # enables or disables the use of the model's internal cache (repetitive output)\n",
    "                        eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "output_ids = generated_ids.numpy().tolist()[0]\n",
    "print(output_ids)\n",
    "\n",
    "# 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(decoded)\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
