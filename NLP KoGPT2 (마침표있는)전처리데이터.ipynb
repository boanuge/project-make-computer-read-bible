{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8862a5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 09:35:30.525249: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-24 09:35:30.525278: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/gbike/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2023-03-24 09:35:38.656290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-24 09:35:38.656319: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-24 09:35:38.656337: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-03-24 09:35:38.656552: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 09:35:38.669126: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.1.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'lm_head.weight', 'transformer.h.5.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.6.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 908876), dtype=int32, numpy=array([[14549,  9347,   387, ...,  9050,  7523,   389]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 908876), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]], dtype=int32)>}\n",
      "Epoch 1/66\n",
      "Step 0 Loss 3.3351941108703613\n",
      "Time duration(in seconds): 14.322258066007635\n",
      "Epoch 2/66\n",
      "Step 0 Loss 3.1992506980895996\n",
      "Time duration(in seconds): 4.187989727011882\n",
      "Epoch 3/66\n",
      "Step 0 Loss 2.6766788959503174\n",
      "Time duration(in seconds): 4.144087071996182\n",
      "Epoch 4/66\n",
      "Step 0 Loss 2.4230310916900635\n",
      "Time duration(in seconds): 4.115100048016757\n",
      "Epoch 5/66\n",
      "Step 0 Loss 2.120518207550049\n",
      "Time duration(in seconds): 3.969981091999216\n",
      "Epoch 6/66\n",
      "Step 0 Loss 1.9098947048187256\n",
      "Time duration(in seconds): 4.00973854699987\n",
      "Epoch 7/66\n",
      "Step 0 Loss 1.7090203762054443\n",
      "Time duration(in seconds): 3.9106705269950908\n",
      "Epoch 8/66\n",
      "Step 0 Loss 1.5147719383239746\n",
      "Time duration(in seconds): 3.9365966329933144\n",
      "Epoch 9/66\n",
      "Step 0 Loss 1.3599551916122437\n",
      "Time duration(in seconds): 3.822288741997909\n",
      "Epoch 10/66\n",
      "Step 0 Loss 1.1884514093399048\n",
      "Time duration(in seconds): 3.9622302250063512\n",
      "Epoch 11/66\n",
      "Step 0 Loss 1.0657281875610352\n",
      "Time duration(in seconds): 3.858920466998825\n",
      "Epoch 12/66\n",
      "Step 0 Loss 0.9398206472396851\n",
      "Time duration(in seconds): 3.874103809997905\n",
      "Epoch 13/66\n",
      "Step 0 Loss 0.8189694881439209\n",
      "Time duration(in seconds): 3.9077028920000885\n",
      "Epoch 14/66\n",
      "Step 0 Loss 0.7027258276939392\n",
      "Time duration(in seconds): 3.952118988003349\n",
      "Epoch 15/66\n",
      "Step 0 Loss 0.6120578050613403\n",
      "Time duration(in seconds): 3.9463896289817058\n",
      "Epoch 16/66\n",
      "Step 0 Loss 0.5331618189811707\n",
      "Time duration(in seconds): 3.9432541109854355\n",
      "Epoch 17/66\n",
      "Step 0 Loss 0.47016534209251404\n",
      "Time duration(in seconds): 3.9119890660222154\n",
      "Epoch 18/66\n",
      "Step 0 Loss 0.3941780924797058\n",
      "Time duration(in seconds): 3.8664328799932264\n",
      "Epoch 19/66\n",
      "Step 0 Loss 0.33827096223831177\n",
      "Time duration(in seconds): 3.8678854320023675\n",
      "Epoch 20/66\n",
      "Step 0 Loss 0.3109578788280487\n",
      "Time duration(in seconds): 3.7801480109919794\n",
      "Epoch 21/66\n",
      "Step 0 Loss 0.24926745891571045\n",
      "Time duration(in seconds): 3.801283601002069\n",
      "Epoch 22/66\n",
      "Step 0 Loss 0.23406988382339478\n",
      "Time duration(in seconds): 3.923157357989112\n",
      "Epoch 23/66\n",
      "Step 0 Loss 0.20287655293941498\n",
      "Time duration(in seconds): 3.7659160069888458\n",
      "Epoch 24/66\n",
      "Step 0 Loss 0.18069197237491608\n",
      "Time duration(in seconds): 3.873024089989485\n",
      "Epoch 25/66\n",
      "Step 0 Loss 0.14598269760608673\n",
      "Time duration(in seconds): 3.72749959200155\n",
      "Epoch 26/66\n",
      "Step 0 Loss 0.13756206631660461\n",
      "Time duration(in seconds): 3.8130160919972695\n",
      "Epoch 27/66\n",
      "Step 0 Loss 0.12437012791633606\n",
      "Time duration(in seconds): 3.7890633960196283\n",
      "Epoch 28/66\n",
      "Step 0 Loss 0.10567262023687363\n",
      "Time duration(in seconds): 3.832550541003002\n",
      "Epoch 29/66\n",
      "Step 0 Loss 0.0977352038025856\n",
      "Time duration(in seconds): 3.758219491021009\n",
      "Epoch 30/66\n",
      "Step 0 Loss 0.09452926367521286\n",
      "Time duration(in seconds): 3.8769223440031055\n",
      "Epoch 31/66\n",
      "Step 0 Loss 0.08454331010580063\n",
      "Time duration(in seconds): 3.822251106001204\n",
      "Epoch 32/66\n",
      "Step 0 Loss 0.07873673737049103\n",
      "Time duration(in seconds): 3.8521176439826377\n",
      "Epoch 33/66\n",
      "Step 0 Loss 0.06628594547510147\n",
      "Time duration(in seconds): 3.926755922002485\n",
      "Epoch 34/66\n",
      "Step 0 Loss 0.057004164904356\n",
      "Time duration(in seconds): 3.8784292570198886\n",
      "Epoch 35/66\n",
      "Step 0 Loss 0.0494120754301548\n",
      "Time duration(in seconds): 3.873886852990836\n",
      "Epoch 36/66\n",
      "Step 0 Loss 0.0484175868332386\n",
      "Time duration(in seconds): 3.842376182001317\n",
      "Epoch 37/66\n",
      "Step 0 Loss 0.0469757579267025\n",
      "Time duration(in seconds): 3.8537793349823914\n",
      "Epoch 38/66\n",
      "Step 0 Loss 0.04010502249002457\n",
      "Time duration(in seconds): 3.7744845549750607\n",
      "Epoch 39/66\n",
      "Step 0 Loss 0.03911791369318962\n",
      "Time duration(in seconds): 3.8111681409936864\n",
      "Epoch 40/66\n",
      "Step 0 Loss 0.037563763558864594\n",
      "Time duration(in seconds): 3.83625363901956\n",
      "Epoch 41/66\n",
      "Step 0 Loss 0.036447275429964066\n",
      "Time duration(in seconds): 3.8551485140051227\n",
      "Epoch 42/66\n",
      "Step 0 Loss 0.02944157086312771\n",
      "Time duration(in seconds): 3.8277908739983104\n",
      "Epoch 43/66\n",
      "Step 0 Loss 0.033737294375896454\n",
      "Time duration(in seconds): 3.8586809870030265\n",
      "Epoch 44/66\n",
      "Step 0 Loss 0.02546113356947899\n",
      "Time duration(in seconds): 3.9121549320116173\n",
      "Epoch 45/66\n",
      "Step 0 Loss 0.02435322478413582\n",
      "Time duration(in seconds): 3.9720130109926686\n",
      "Epoch 46/66\n",
      "Step 0 Loss 0.020847048610448837\n",
      "Time duration(in seconds): 3.8114289799996186\n",
      "Epoch 47/66\n",
      "Step 0 Loss 0.02097434550523758\n",
      "Time duration(in seconds): 3.8570299050188623\n",
      "Epoch 48/66\n",
      "Step 0 Loss 0.018960176035761833\n",
      "Time duration(in seconds): 3.806747874012217\n",
      "Epoch 49/66\n",
      "Step 0 Loss 0.018643701449036598\n",
      "Time duration(in seconds): 3.803841538989218\n",
      "Epoch 50/66\n",
      "Step 0 Loss 0.01805475726723671\n",
      "Time duration(in seconds): 4.260125144995982\n",
      "Epoch 51/66\n",
      "Step 0 Loss 0.023363430052995682\n",
      "Time duration(in seconds): 4.306825525010936\n",
      "Epoch 52/66\n",
      "Step 0 Loss 0.021438593044877052\n",
      "Time duration(in seconds): 4.350120896997396\n",
      "Epoch 53/66\n",
      "Step 0 Loss 0.022105839103460312\n",
      "Time duration(in seconds): 4.204508219991112\n",
      "Epoch 54/66\n",
      "Step 0 Loss 0.015611608512699604\n",
      "Time duration(in seconds): 3.820316930010449\n",
      "Epoch 55/66\n",
      "Step 0 Loss 0.01366758719086647\n",
      "Time duration(in seconds): 3.6781645190203562\n",
      "Epoch 56/66\n",
      "Step 0 Loss 0.012326488271355629\n",
      "Time duration(in seconds): 3.810816846002126\n",
      "Epoch 57/66\n",
      "Step 0 Loss 0.01186725776642561\n",
      "Time duration(in seconds): 3.8697633869887795\n",
      "Epoch 58/66\n",
      "Step 0 Loss 0.012897155247628689\n",
      "Time duration(in seconds): 3.766077200998552\n",
      "Epoch 59/66\n",
      "Step 0 Loss 0.012066638097167015\n",
      "Time duration(in seconds): 3.898960888007423\n",
      "Epoch 60/66\n",
      "Step 0 Loss 0.014365791343152523\n",
      "Time duration(in seconds): 3.862586064002244\n",
      "Epoch 61/66\n",
      "Step 0 Loss 0.011119386181235313\n",
      "Time duration(in seconds): 3.8148715640127193\n",
      "Epoch 62/66\n",
      "Step 0 Loss 0.015335487201809883\n",
      "Time duration(in seconds): 3.8210874520009384\n",
      "Epoch 63/66\n",
      "Step 0 Loss 0.011722767725586891\n",
      "Time duration(in seconds): 4.100666106998688\n",
      "Epoch 64/66\n",
      "Step 0 Loss 0.01060979999601841\n",
      "Time duration(in seconds): 4.325153843994485\n",
      "Epoch 65/66\n",
      "Step 0 Loss 0.010917751118540764\n",
      "Time duration(in seconds): 4.2820774799911305\n",
      "Epoch 66/66\n",
      "Step 0 Loss 0.013985570520162582\n",
      "Time duration(in seconds): 4.502413239009911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('kogpt2-finetuned-epoch-66/tokenizer_config.json',\n",
       " 'kogpt2-finetuned-epoch-66/special_tokens_map.json',\n",
       " 'kogpt2-finetuned-epoch-66/vocab.json',\n",
       " 'kogpt2-finetuned-epoch-66/merges.txt',\n",
       " 'kogpt2-finetuned-epoch-66/added_tokens.json',\n",
       " 'kogpt2-finetuned-epoch-66/tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPTv2 모델 파인튜닝 & 저장 : 약 3시간 걸림 (8 CPU & 0 GPU)\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('한글성경(마침표있음)_정제후말뭉치_오십만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the GPT-2 model\n",
    "model = TFGPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2', from_pt=True)\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "# Define the training parameters\n",
    "model_path = \"./output/kogpt2-마침표있음-finetuned-epoch-66\"\n",
    "learning_rate = 3e-5\n",
    "batch_size = 16\n",
    "epochs = 66\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # \"1024\" for GPT-2\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3647bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델 로딩 시간]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/kogpt2-마침표있음-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time duration(in seconds): 1.886370230000466\n",
      "[문장 생성 시간]\n",
      "Prompt: 예수님\n",
      "Generated text: 께서 또 말씀하셨습니다. 빛이 생겨라! 그러자 빛과 어둠이 생겼습니다.\n",
      "그 때에 하나님의 영은 물 위에서 움직이고 계셔서 물을 둘로 나누어라.\n",
      "하나님은 빛을 낮 이라 부르시고, 그 빛들이 밤 이브라힘을 만드셨다.\n",
      "저녁과 아침, 그리고 저녁은 하늘과 땅 위에 놓여 있었습니다\n",
      "(출애굽기 25:12). 새벽 두 시경, 안식일, 그리하여 하늘 아래의 모든 것들이 다 지어졌지요.\n",
      "그러나 해가 뉘엿뉘그레지고 날이 되니, 이제야 일이 풀렸네요.\n",
      "일곱째 날, 한빛님이 하시던 일을 마치시고 쉬었 았다 가겠어요.\n",
      "한빛이 일곱 번째 날이었죠.\n",
      "처음 창조된 남자, 하늘에 빛나는 별들을 보시기에 좋았는데요. 오늘도 큰 추위가 이어지겠지만, 내일은 다시 영하로 떨어집니다.\n",
      "내륙에는 안개가 올라와 온 나라를 뒤흔들 것으로 보입니다.\n",
      "자세한 날씨 기상캐스터 연결해서 알아보도록 하겠니다.\n",
      "김민혜 캐s. 나와 있습니다.\n",
      "네.\n",
      "Time duration(in seconds): 64.62919808900915\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 및 테스트\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "print(\"[모델 로딩 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "loaded_model = TFGPT2LMHeadModel.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print(\"[문장 생성 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "prompt = \"예수님\"\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                   max_length=200+input_ids.shape[1],\n",
    "                                   num_beams=1, # Higher value increases the computational cost\n",
    "                                   no_repeat_ngram_size=1, # Higher means more repetitive words\n",
    "                                   repetition_penalty=2.0, # Higher avoids repeating sentences\n",
    "                                   temperature=0.1, # Higher means more diverse : 0 ~ 1\n",
    "                                   top_p=0.9, # Lower means more diverse : 0 ~ 1\n",
    "                                   early_stopping=True)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt: \" + prompt)\n",
    "\n",
    "split_generated_text = generated_text.split(prompt)\n",
    "if len(split_generated_text) > 1:\n",
    "    generated_text = split_generated_text[1]\n",
    "# Trim the sentences after the last period(.)\n",
    "text_to_remove = generated_text.split('.')[-1]\n",
    "generated_text = generated_text.replace(text_to_remove,'')\n",
    "print(\"Generated text: \" + generated_text)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8ed28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델 로딩 시간]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/kogpt2-마침표있음-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time duration(in seconds): 2.1700137660081964\n",
      "[문장 생성 시간]\n",
      "Prompt: 예수님\n",
      "Generated text: 께서 또 말씀하셨습니다. 빛이 생겨라! 그러자 빛이 생겼습니다.\n",
      "그 빛이 하나님께서 보시기에 좋았습니다. 하나님께서 빛과 어둠을 나누셨습니다.\n",
      "하나님께서는 빛을 낮 이라 부르시고, 어둠을 밤 이라 부르셨습니다. 저녁이 지나고 아침이 되니, 이 날이 첫째 날이었습니다.\n",
      "하나님께서 또 말씀하셨습니다. 물 한가운데 둥근 공간이 생겨 물을 둘로 나누어라.\n",
      "하나님께서 둥근 공간을 만드시고, 그 공간 아래의 물과 공간 위의 물을 나누시니 그대로 되었습니다.\n",
      "하나님께서 그 공간을 하늘 이라 부르셨습니다. 저녁이 지나고 아침이 되니, 이 날이 둘째 날이었습니다.\n",
      "하나님께서 말씀하셨습니다. 하늘 아래의 물은 한 곳으로 모이고 뭍은 드러나라 하시니 그대로 되었습니다.\n",
      "하나님께서 뭍을 땅 이라 부르시고 모인 물은 바다 라고 부르셨습니다. 하나님께서 보시기에 좋았습니다.\n",
      "하나님께서 말씀하셨습니다. 땅은 풀과 씨를 맺는 식물과 씨가 든 열매를 맺는 온갖 과일나무를 내어라 하시니, 그대로 되었습니다.\n",
      "Time duration(in seconds): 50.014406750007765\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 및 테스트\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "print(\"[모델 로딩 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "loaded_model = TFGPT2LMHeadModel.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print(\"[문장 생성 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "prompt = \"예수님\"\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                   max_length=200+input_ids.shape[1],\n",
    "                                   early_stopping=True)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt: \" + prompt)\n",
    "\n",
    "split_generated_text = generated_text.split(prompt)\n",
    "if len(split_generated_text) > 1:\n",
    "    generated_text = split_generated_text[1]\n",
    "# Trim the sentences after the last period(.)\n",
    "text_to_remove = generated_text.split('.')[-1]\n",
    "generated_text = generated_text.replace(text_to_remove,'')\n",
    "print(\"Generated text: \" + generated_text)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f991709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[모델 로딩 시간]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./output/kogpt2-마침표있음-finetuned-epoch-66.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time duration(in seconds): 1.8056226579938084\n",
      "[문장 생성 시간]\n",
      "Prompt: 예수님\n",
      "Generated text: 께서 또 말씀하셨습니다. 빛이 생겨라! 그러자 빛이 생겼습니다.\n",
      "그 빛이 하나님께서 보시기에 좋았습니다.\n",
      "Time duration(in seconds): 7.202055873000063\n"
     ]
    }
   ],
   "source": [
    "# 모델 로드 및 테스트\n",
    "from transformers import TFGPT2LMHeadModel, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "print(\"[모델 로딩 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "loaded_model = TFGPT2LMHeadModel.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./output/kogpt2-마침표있음-finetuned-epoch-66\")\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "print(\"[문장 생성 시간]\")\n",
    "start = default_timer()\n",
    "\n",
    "prompt = \"예수님\"\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                   max_length=30+input_ids.shape[1],\n",
    "                                   early_stopping=True)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Prompt: \" + prompt)\n",
    "\n",
    "split_generated_text = generated_text.split(prompt)\n",
    "if len(split_generated_text) > 1:\n",
    "    generated_text = split_generated_text[1]\n",
    "# Trim the sentences after the last period(.)\n",
    "text_to_remove = generated_text.split('.')[-1]\n",
    "generated_text = generated_text.replace(text_to_remove,'')\n",
    "print(\"Generated text: \" + generated_text)\n",
    "\n",
    "end = default_timer()\n",
    "print(\"Time duration(in seconds):\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "def ouputGPT(prompt=\"\"):\n",
    "    print(\"[문장 생성 시간]\")\n",
    "    start = default_timer()\n",
    "\n",
    "    #prompt = \"예수님\"\n",
    "    input_ids = loaded_tokenizer.encode(prompt, return_tensors=\"tf\")\n",
    "    output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                       max_length=30+input_ids.shape[1],\n",
    "                                       early_stopping=True)\n",
    "    generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Prompt: \" + prompt)\n",
    "\n",
    "    split_generated_text = generated_text.split(prompt)\n",
    "    if len(split_generated_text) > 1:\n",
    "        generated_text = split_generated_text[1]\n",
    "    # Trim the sentences after the last period(.)\n",
    "    text_to_remove = generated_text.split('.')[-1]\n",
    "    generated_text = generated_text.replace(text_to_remove,'')\n",
    "    print(\"Generated text: \" + generated_text)\n",
    "\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Let's chat for 10 lines\n",
    "for step in range(10):\n",
    "    prompt = input(\">> User:\")\n",
    "    if prompt.lower() == \"bye\": break\n",
    "    print(\"GPT: {}\".format( ouputGPT(prompt) ))\n",
    "    print_current_datetime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
