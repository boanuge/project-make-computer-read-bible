{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained microsoft/DialoGPT-large @ CDT(2023-04-03T20:46:16.762737)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0d681fbf81465bba5f79b86c9096e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\A\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa4ed15fcb84668a3488a7fce298077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de6d6592604903ab378862eb34f6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35538ba5352466f947f17934f437db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c26104d454a4549b22af7f1271a37fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/3.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8222207747e4924bf36ac99b02bc66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-03T20:51:42.969294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi and I'm a big fan of your videos\n",
      "Response 1: Hi and I'm a big fan of your videos\n",
      "Response 2: Hi\n",
      "Response 3: Hi\n",
      "Response 4: Hi\n",
      "Response 5: Hi\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Loading pre-trained microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids,\n",
    "    max_length=1024,\n",
    "    temperature=0.7, # This parameter controls the \"creativity\" of the generated text, \"1\" results unpredictable text\n",
    "    top_p=0.7, # Cumulative probability distribution of the next word, \"top_k\": limits the number of candidate words \n",
    "    do_sample=True, # Random sampling of the next token instead of selecting the token with the highest probability \n",
    "    num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "print(loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "# The below code loops through the generated responses and print them out with a response number.\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:30:15.742516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-04 07:30:15.742551: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T07:30:17.126496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:30:17.926118: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-04 07:30:17.926146: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-04 07:30:17.926165: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-04-04 07:30:17.926362: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 07:30:17.938886: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated-text:  you, welcome to the club!\n",
      "Response 1: Hi you, welcome to the club!\n",
      "Response 2: Hi\n",
      "Response 3: Hi\n",
      "Response 4: Hi.\n",
      "Response 5: Hi :D\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "split_generated_text = generated_text.split(prompt)\n",
    "if len(split_generated_text) > 1:\n",
    "    generated_text = split_generated_text[1]\n",
    "\n",
    "print(\"Generated-text:\",generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune(Before) the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T07:51:44.185421)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus @ CDT(2023-04-04T07:52:15.653434)\n",
      "Response 1: Jesus\n",
      "Response 2: Jesus... the title was a little unclear.\n",
      "Response 3: Jesus\n",
      "Response 4: Jesus a... it's a trap.\n",
      "Response 5: Jesus, is there any chance you could turn off the game from your home computer. It is your PC's fault, you can't make a computer turn itself on.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Finetune(Before) the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Jesus\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print_current_datetime(generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3054759 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 3054759), dtype=int32, numpy=array([[ 818,  262, 3726, ...,  345,  477,   13]])>, 'attention_mask': <tf.Tensor: shape=(1, 3054759), dtype=int32, numpy=array([[1, 1, 1, ..., 1, 1, 1]])>}\n",
      "Finetune the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T17:47:04.236880)\n",
      "Epoch 1/6\n",
      "Step 0 Loss 12.522553443908691\n",
      "Time duration(in seconds): 74.71662520000001\n",
      "Epoch 2/6\n",
      "Step 0 Loss 10.721572875976562\n",
      "Time duration(in seconds): 30.79476849999999\n",
      "Epoch 3/6\n",
      "Step 0 Loss 10.05924129486084\n",
      "Time duration(in seconds): 31.29861169999998\n",
      "Epoch 4/6\n",
      "Step 0 Loss 9.5293607711792\n",
      "Time duration(in seconds): 31.979575799999992\n",
      "Epoch 5/6\n",
      "Step 0 Loss 9.15305233001709\n",
      "Time duration(in seconds): 31.681460799999996\n",
      "Epoch 6/6\n",
      "Step 0 Loss 8.982477188110352\n",
      "Time duration(in seconds): 31.39858380000001\n",
      " @ CDT(2023-04-04T17:52:13.947517)\n",
      "Finetune(After) the DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T17:52:13.948516)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jesus @ CDT(2023-04-04T17:52:32.831162)\n",
      "Response 1: Jesus\n",
      "Response 2: Jesus the that's not how it works.\n",
      "Response 3: Jesus on dude\n",
      "Response 4: Jesus!, i want to learn Spanish\n",
      "Response 5: Jesus\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "from timeit import default_timer\n",
    "\n",
    "# Load the text data\n",
    "with open('영어성경(NKJV+NIV+NLT)_이백만단어.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Instantiate the model\n",
    "model = TFAutoModelForCausalLM.from_pretrained('DialoGPT-large-by-Microsoft-Keras')\n",
    "\n",
    "# Instantiate the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('DialoGPT-large-by-Microsoft-Keras')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_text = tokenizer(text, return_tensors='tf')\n",
    "print(tokenized_text)\n",
    "\n",
    "print_current_datetime(\"Finetune the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "# Define the training parameters\n",
    "model_path = \"DialoGPT-large-by-Microsoft-Keras-finetuned\"\n",
    "learning_rate = 1e-5\n",
    "batch_size = 6\n",
    "epochs = 6\n",
    "\n",
    "# Define the training function\n",
    "@tf.function\n",
    "def train_step(input_ids):\n",
    "    # Truncate input sequence\n",
    "    max_seq_length = 1024 # \"1024\" for DialoGPT-large\n",
    "    input_ids = input_ids[:, :max_seq_length]\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(input_ids, training=True)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = input_ids[:, 1:]\n",
    "        loss_value = loss(labels, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    return loss_value\n",
    "\n",
    "# Create a TensorSliceDataset from the tokenized text\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tokenized_text['input_ids'])\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Fine-tune the model\n",
    "for epoch in range(epochs):\n",
    "    start = default_timer()\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    for step, batch in enumerate(dataset):\n",
    "        loss_value = train_step(batch)\n",
    "        if step % 50 == 0:\n",
    "            print(f'Step {step} Loss {loss_value}')\n",
    "    end = default_timer()\n",
    "    print(\"Time duration(in seconds):\", end - start)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "print_current_datetime()\n",
    "\n",
    "print_current_datetime(\"Finetune(After) the DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras-finetuned\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras-finetuned\")\n",
    "\n",
    "prompt = \"Jesus\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print_current_datetime(generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
