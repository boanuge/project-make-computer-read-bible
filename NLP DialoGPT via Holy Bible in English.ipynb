{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained microsoft/DialoGPT-large @ CDT(2023-04-03T20:46:16.762737)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0d681fbf81465bba5f79b86c9096e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Roaming\\Python\\Python39\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\A\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa4ed15fcb84668a3488a7fce298077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de6d6592604903ab378862eb34f6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35538ba5352466f947f17934f437db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c26104d454a4549b22af7f1271a37fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/3.10G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8222207747e4924bf36ac99b02bc66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-03T20:51:42.969294)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi and I'm a big fan of your videos\n",
      "Response 1: Hi and I'm a big fan of your videos\n",
      "Response 2: Hi\n",
      "Response 3: Hi\n",
      "Response 4: Hi\n",
      "Response 5: Hi\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Loading pre-trained microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "\n",
    "tokenizer.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "model.save_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids,\n",
    "    max_length=1024,\n",
    "    temperature=0.7, # This parameter controls the \"creativity\" of the generated text, \"1\" results unpredictable text\n",
    "    top_p=0.7, # Cumulative probability distribution of the next word, \"top_k\": limits the number of candidate words \n",
    "    do_sample=True, # Random sampling of the next token instead of selecting the token with the highest probability \n",
    "    num_return_sequences=5, # The model will generate five different responses to the prompt.\n",
    "    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "print(loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# num_return_sequences=5, which means the model will generate 5 different responses to the prompt.\n",
    "# The below code loops through the generated responses and print them out with a response number.\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:30:15.742516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-04-04 07:30:15.742551: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved DialoGPT-large-by-Microsoft-Keras @ CDT(2023-04-04T07:30:17.126496)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 07:30:17.926118: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-04 07:30:17.926146: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-04 07:30:17.926165: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-255-120-161): /proc/driver/nvidia/version does not exist\n",
      "2023-04-04 07:30:17.926362: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 07:30:17.938886: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at DialoGPT-large-by-Microsoft-Keras.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated-text:  you, welcome to the club!\n",
      "Response 1: Hi you, welcome to the club!\n",
      "Response 2: Hi\n",
      "Response 3: Hi\n",
      "Response 4: Hi.\n",
      "Response 5: Hi :D\n"
     ]
    }
   ],
   "source": [
    "# Microsoft/DialoGPT-large(file size = 3GB) supports multiple languages\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "import tensorflow as tf\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "# Print the current date and time in the format:\n",
    "# \"YYYY-MM-DD HH:MM:SS.microseconds\"\n",
    "import datetime\n",
    "def print_current_datetime(text=\"\"):\n",
    "    datetime_string = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S.%f\")\n",
    "    print(\"{} @ CDT({})\".format(text,datetime_string))\n",
    "\n",
    "print_current_datetime(\"Loading saved DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "loaded_model = TFAutoModelForCausalLM.from_pretrained(\"DialoGPT-large-by-Microsoft-Keras\")\n",
    "\n",
    "prompt = \"Hi\"\n",
    "\n",
    "input_ids = loaded_tokenizer.encode(prompt, return_tensors='tf')\n",
    "output_ids = loaded_model.generate(input_ids=input_ids,\n",
    "                                    max_length=1024+input_ids.shape[1],\n",
    "                                    temperature=0.9,\n",
    "                                    top_p=0.9,\n",
    "                                    do_sample=True,\n",
    "                                    num_return_sequences=5,\n",
    "                                    pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "generated_text = loaded_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "split_generated_text = generated_text.split(prompt)\n",
    "if len(split_generated_text) > 1:\n",
    "    generated_text = split_generated_text[1]\n",
    "\n",
    "print(\"Generated-text:\",generated_text)\n",
    "\n",
    "for i, output_sequence in enumerate(output_ids):\n",
    "    print(f'Response {i+1}: {loaded_tokenizer.decode(output_sequence, skip_special_tokens=True)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
